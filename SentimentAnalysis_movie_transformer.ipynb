{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "SentimentAnalysis_movie_transformer",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1dmHFwTj7F5",
        "outputId": "1dff3629-75a7-497e-8f91-30abb9f87231"
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "from konlpy.tag import Komoran, Okt\n",
        "from tqdm import tqdm, trange\n",
        "import sentencepiece as spm\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0UL92rXDj7F6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5facd9e8-8849-4f3f-bb90-bb0d79318ef6"
      },
      "source": [
        "%matplotlib inline\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(515)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff954b93a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2Z_pjQNGlYf",
        "outputId": "18c800db-1e3b-40c1-999c-dc178b1304de"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLowTOjvDcl0",
        "outputId": "6f6bdd53-288b-4f8c-d145-698c02c3147e"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aG9COpUj7F-"
      },
      "source": [
        "# preprocessing\n",
        "def load_vocab(file):\n",
        "    vocab = spm.SentencePieceProcessor()\n",
        "    vocab.load(file)\n",
        "    return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwWfCmNdj7F-"
      },
      "source": [
        "# utils\n",
        "def train(model, criterion, optimizer, input_size, output_size,\n",
        "          train_dataloader, validation_dataloader,\n",
        "          nb_epochs):\n",
        "    \n",
        "    # Train loop\n",
        "    trn_loss_list = []\n",
        "    val_loss_list = []\n",
        "    for epoch in range(nb_epochs):\n",
        "\n",
        "        # Train\n",
        "        trn_loss = 0.0\n",
        "        for trn_batch, train_samples in enumerate(train_dataloader):\n",
        "\n",
        "            # train data setting\n",
        "            x_train, y_train = train_samples\n",
        "            x_train = x_train.unsqueeze(0).to(device)\n",
        "            y_train = y_train.float().to(device)\n",
        "\n",
        "            # train\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            hypothesis = model(x_train).view(-1, output_size)\n",
        "            train_loss = criterion(hypothesis, y_train)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\n",
        "            # train loss\n",
        "            trn_loss += train_loss.item() / len(train_dataloader)\n",
        "        trn_loss_list.append(trn_loss)\n",
        "\n",
        "        # Evaluation\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0.0\n",
        "            for val_batch, validation_samples in enumerate(validation_dataloader):\n",
        "\n",
        "                # validatoin data setting\n",
        "                x_validation, y_validation = validation_samples\n",
        "                x_validation = x_validation.unsqueeze(0).to(device)\n",
        "                y_validation = y_validation.float().to(device)\n",
        "\n",
        "                # evaluation\n",
        "                model.eval()\n",
        "                prediction = model(x_validation).view(-1, output_size)\n",
        "                validation_loss = criterion(prediction, y_validation)\n",
        "\n",
        "                # validation loss\n",
        "                val_loss += validation_loss.item() / len(validation_dataloader)\n",
        "            val_loss_list.append(val_loss)\n",
        "\n",
        "        print(\"Epoch: {:3d} | Train Loss: {:.6f} | Val Loss: {:.6f}\".format(epoch + 1, trn_loss, val_loss))\n",
        "    #torch.save(model, './data/temperature_model.pt')\n",
        "    return trn_loss_list, val_loss_list\n",
        "\n",
        "def checkdata(dataloader):\n",
        "    for batch, values in enumerate(dataloader):\n",
        "        while batch < 6:\n",
        "            label, enc_inputs, dec_inputs = values\n",
        "            print(\"{} Batch\".format(batch))\n",
        "            print(\"Input: {}\".format(enc_inputs.size()))\n",
        "            print(\"Target: {}\".format(dec_inputs.size()))\n",
        "            break\n",
        "    return label, enc_inputs, dec_inputs\n",
        "\n",
        "def checkfunction(criterion, dataloader, output_size, epoch):\n",
        "    for batch, value in enumerate(dataloader):\n",
        "        x_, y_ = value\n",
        "        x_ = x_.unsqueeze(0).to(device)\n",
        "        y_ = y_.float().to(device)\n",
        "        hypothesis = model(x_).view(-1, output_size)\n",
        "        loss = criterion(hypothesis, y_)\n",
        "        print(\"Batch:\", batch+1)\n",
        "        print(\"Input size:\", x_.size())\n",
        "        print(\"Target size:\", y_.size())\n",
        "        print(\"Hypothesis\", hypothesis)\n",
        "        print(\"Hypothesis size:\", hypothesis.size())\n",
        "        print(\"Loss:\", loss)\n",
        "        print()\n",
        "        if batch == epoch:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh20NkAJQbxP"
      },
      "source": [
        "class Config(dict): \r\n",
        "    __getattr__ = dict.__getitem__\r\n",
        "    __setattr__ = dict.__setitem__\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def load(cls, file):\r\n",
        "        with open(file, 'r') as f:\r\n",
        "            config = json.loads(f.read())\r\n",
        "            return Config(config)\r\n",
        "\r\n",
        "class CustomDataSet(data.Dataset):\r\n",
        "    def __init__(self, vocab, infile):\r\n",
        "\r\n",
        "        self.vocab = vocab\r\n",
        "        self.labels = []\r\n",
        "        self.sentences = []\r\n",
        "\r\n",
        "        line_cnt = 0\r\n",
        "        with open(infile, \"r\") as f:\r\n",
        "            for line in f:\r\n",
        "                line_cnt += 1\r\n",
        "\r\n",
        "        with open(infile, \"r\") as f:\r\n",
        "            for i, line in enumerate(tqdm(f, total=line_cnt, desc=f\"Loading {infile}\", unit=\" lines\")):\r\n",
        "                data = json.loads(line)\r\n",
        "                self.labels.append(data[\"label\"])\r\n",
        "                self.sentences.append([vocab.piece_to_id(p) for p in data[\"doc\"]])\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        assert len(self.labels) == len(self.sentences)\r\n",
        "        return len(self.labels)\r\n",
        "    \r\n",
        "    def __getitem__(self, item):\r\n",
        "        return (torch.tensor(self.labels[item]),\r\n",
        "                torch.tensor(self.sentences[item]),\r\n",
        "                torch.tensor([self.vocab.piece_to_id(\"[BOS]\")]))\r\n",
        "        \r\n",
        "def movie_collate_fn(inputs):\r\n",
        "    labels, enc_inputs, dec_inputs = list(zip(*inputs))\r\n",
        "\r\n",
        "    enc_inputs = torch.nn.utils.rnn.pad_sequence(enc_inputs, batch_first=True, padding_value=0)\r\n",
        "    dec_inputs = torch.nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True, padding_value=0)\r\n",
        "\r\n",
        "    batch = [\r\n",
        "        torch.stack(labels, dim=0),\r\n",
        "        enc_inputs,\r\n",
        "        dec_inputs,\r\n",
        "    ]\r\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr93JtZd9K-M"
      },
      "source": [
        "# Model\r\n",
        "class Classification(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        # values\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        # layers\r\n",
        "        self.transformer = Transformer(self.config)\r\n",
        "        self.projection = nn.Linear(self.config['hidden_size'], self.config['output_size'], bias = False)\r\n",
        "\r\n",
        "    def forward(self, enc_inputs, dec_inputs):\r\n",
        "        dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs = self.transformer(enc_inputs, dec_inputs)\r\n",
        "        dec_outputs, _ = torch.max(dec_outputs, dim=1)\r\n",
        "        logits = self.projection(dec_outputs)\r\n",
        "        return logits, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs\r\n",
        "\r\n",
        "    def save(self, epoch, loss, score, path):\r\n",
        "        torch.save({\r\n",
        "            \"epoch\": epoch,\r\n",
        "            \"loss\": loss,\r\n",
        "            \"score\": score,\r\n",
        "            \"state_dict\": self.state_dict()\r\n",
        "        }, path)\r\n",
        "    \r\n",
        "    def load(self, path):\r\n",
        "        save = torch.load(path)\r\n",
        "        self.load_state_dict(save[\"state_dict\"])\r\n",
        "        return save[\"epoch\"], save[\"loss\"], save[\"score\"]\r\n",
        "\r\n",
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # values\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        # layers\r\n",
        "        self.encoder = Encoder(self.config)\r\n",
        "        self.decoder = Decoder(self.config)\r\n",
        "\r\n",
        "    def forward(self, enc_inputs, dec_inputs):\r\n",
        "        enc_outputs, enc_self_attn_probs = self.encoder(enc_inputs)\r\n",
        "        dec_outputs, dec_self_attn_probs, dec_enc_attn_probs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\r\n",
        "        return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7rsH1HFd_ni"
      },
      "source": [
        "# Encoding\r\n",
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # values\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        # layers\r\n",
        "        self.enc_emb = nn.Embedding(self.config['num_encoding_vocab'], self.config['hidden_size'])\r\n",
        "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config['num_encoding_sequence'] + 1, self.config['hidden_size']))\r\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze = True)\r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config['num_layers'])])\r\n",
        "  \r\n",
        "    def forward(self, x):\r\n",
        "        positions = torch.arange(x.size(1), device=x.device, dtype=x.dtype).expand(x.size(0), x.size(1)).contiguous() + 1\r\n",
        "        pos_mask = x.eq(self.config['i_pad'])\r\n",
        "        outputs = self.enc_emb(x) + self.pos_emb(positions)\r\n",
        "        attn_mask = get_attn_pad_mask(x, x, self.config['i_pad'])\r\n",
        "\r\n",
        "        attn_probs = []\r\n",
        "        for layer in self.layers:\r\n",
        "            outputs, attn_prob = layer(outputs, attn_mask)\r\n",
        "            attn_probs.append(attn_prob)\r\n",
        "        return outputs, attn_probs\r\n",
        "\r\n",
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # values\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        # layers\r\n",
        "        self.self_attn = MultiHeadAttention(self.config)\r\n",
        "        self.layer_norm1 = nn.LayerNorm(self.config['hidden_size'], eps=self.config['layer_norm_epsilon'])\r\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\r\n",
        "        self.layer_norm2 = nn.LayerNorm(self.config['hidden_size'], eps=self.config['layer_norm_epsilon'])\r\n",
        "\r\n",
        "    def forward(self, x, attn_mask):\r\n",
        "        att_outputs, attn_prob = self.self_attn(x, x, x, attn_mask)\r\n",
        "        att_outputs = self.layer_norm1(x + att_outputs)\r\n",
        "\r\n",
        "        ffn_outputs = self.pos_ffn(att_outputs)\r\n",
        "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\r\n",
        "        return ffn_outputs, attn_prob\r\n",
        "\r\n",
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # values\r\n",
        "        self.config = config\r\n",
        "        \r\n",
        "        # layers\r\n",
        "        self.W_Q = nn.Linear(self.config['hidden_size'], self.config['n_head'] * self.config['d_head'])\r\n",
        "        self.W_K = nn.Linear(self.config['hidden_size'], self.config['n_head'] * self.config['d_head'])\r\n",
        "        self.W_V = nn.Linear(self.config['hidden_size'], self.config['n_head'] * self.config['d_head'])\r\n",
        "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\r\n",
        "        self.linear = nn.Linear(self.config['n_head'] * self.config['d_head'], self.config['hidden_size'])\r\n",
        "        self.dropout = nn.Dropout(self.config['dropout'])\r\n",
        "\r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        batch_size = Q.size(0)\r\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.config['n_head'], self.config['d_head']).transpose(1,2)\r\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.config['n_head'], self.config['d_head']).transpose(1,2)\r\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.config['n_head'], self.config['d_head']).transpose(1,2)\r\n",
        "\r\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config['n_head'], 1, 1)\r\n",
        "\r\n",
        "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\r\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config['n_head'] * self.config['d_head'])\r\n",
        "        output = self.linear(context)\r\n",
        "        output = self.dropout(output)\r\n",
        "        return output, attn_prob\r\n",
        "\r\n",
        "class PoswiseFeedForwardNet(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        # values\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        # layers\r\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.config['hidden_size'], out_channels=self.config['d_ff'], kernel_size=1)\r\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.config['d_ff'], out_channels=self.config['hidden_size'], kernel_size=1)\r\n",
        "        self.active = F.gelu\r\n",
        "        self.dropout = nn.Dropout(self.config['dropout'])\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        output = self.active(self.conv1(x.transpose(1, 2)))\r\n",
        "        output = self.conv2(output).transpose(1, 2)\r\n",
        "        output = self.dropout(output)\r\n",
        "        return output\r\n",
        "\r\n",
        "class ScaledDotProductAttention(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # values\r\n",
        "        self.config = config\r\n",
        "        self.dropout = nn.Dropout(self.config['dropout'])\r\n",
        "        self.scale = 1 / (self.config['d_head'] ** 0.5)\r\n",
        "\r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\r\n",
        "        scores.masked_fill_(attn_mask, -1e9)\r\n",
        "        attn_prob = nn.Softmax(dim=-1)(scores)\r\n",
        "        attn_prob = self.dropout(attn_prob)\r\n",
        "        context = torch.matmul(attn_prob, V)\r\n",
        "        return context, attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "filyb2va6XqV"
      },
      "source": [
        "# Decoding\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # values\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        # layers\r\n",
        "        self.dec_embs = nn.Embedding(self.config['num_decoding_vocab'], self.config['hidden_size'])\r\n",
        "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config['num_decoding_sequence'] + 1, self.config['hidden_size']))\r\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config['num_layers'])])\r\n",
        "\r\n",
        "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\r\n",
        "        positions = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\r\n",
        "        pos_mask = dec_inputs.eq(self.config['i_pad'])\r\n",
        "        positions.masked_fill_(pos_mask, 0)\r\n",
        "    \r\n",
        "        dec_outputs = self.dec_embs(dec_inputs) + self.pos_emb(positions)\r\n",
        "\r\n",
        "        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config['i_pad'])\r\n",
        "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)\r\n",
        "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\r\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs, self.config['i_pad'])\r\n",
        "\r\n",
        "        self_attn_probs, dec_enc_attn_probs = [], []\r\n",
        "        for layer in self.layers:\r\n",
        "            dec_outputs, self_attn_prob, dec_enc_attn_prob = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\r\n",
        "            self_attn_probs.append(self_attn_prob)\r\n",
        "            dec_enc_attn_probs.append(dec_enc_attn_prob)\r\n",
        "        return dec_outputs, self_attn_probs, dec_enc_attn_probs\r\n",
        "\r\n",
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # values\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        # layers\r\n",
        "        self.self_attn = MultiHeadAttention(self.config)\r\n",
        "        self.layer_norm1 = nn.LayerNorm(self.config['hidden_size'], eps=self.config['layer_norm_epsilon'])\r\n",
        "        self.dec_enc_attn = MultiHeadAttention(self.config)\r\n",
        "        self.layer_norm2 = nn.LayerNorm(self.config['hidden_size'], eps=self.config['layer_norm_epsilon'])\r\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\r\n",
        "        self.layer_norm3 = nn.LayerNorm(self.config['hidden_size'], eps=self.config['layer_norm_epsilon'])\r\n",
        "    \r\n",
        "    def forward(self, dec_inputs, enc_outputs, self_attn_mask, dec_enc_attn_mask):\r\n",
        "        self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\r\n",
        "        self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs)\r\n",
        "        dec_enc_att_outputs, dec_enc_attn_prob = self.dec_enc_attn(self_att_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\r\n",
        "        dec_enc_att_outputs = self.layer_norm2(self_att_outputs + dec_enc_att_outputs)\r\n",
        "        ffn_outputs = self.pos_ffn(dec_enc_att_outputs)\r\n",
        "        ffn_outputs = self.layer_norm3(dec_enc_att_outputs + ffn_outputs)\r\n",
        "        return ffn_outputs, self_attn_prob, dec_enc_attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH7JAWPRBrz_"
      },
      "source": [
        "# Encoding/Decoding util functions\r\n",
        "def get_sinusoid_encoding_table(n_seq, hidden_size):\r\n",
        "    def cal_angle(position, i_hidn):\r\n",
        "        return position / np.power(10000, 2 * (i_hidn // 2) / hidden_size)\r\n",
        "    def get_posi_angle_vec(position):\r\n",
        "        return [cal_angle(position, i_hidn) for i_hidn in range(hidden_size)]\r\n",
        "\r\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\r\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \r\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\r\n",
        "    return sinusoid_table\r\n",
        "\r\n",
        "def get_attn_pad_mask(seq_q, seq_k, i_pad):\r\n",
        "    batch_size, len_q = seq_q.size()\r\n",
        "    batch_size, len_k = seq_k.size()\r\n",
        "    pad_attn_mask = seq_k.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)\r\n",
        "    return pad_attn_mask\r\n",
        "\r\n",
        "def get_attn_decoder_mask(seq):\r\n",
        "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\r\n",
        "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\r\n",
        "    return subsequent_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqjI23Y1EPqV"
      },
      "source": [
        "vocab = load_vocab(\"/content/drive/My Drive/data/kowiki.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAWbHNs27AQW"
      },
      "source": [
        "config = Config({\r\n",
        "    \"batch_size\": 64, \r\n",
        "    \"num_encoding_vocab\": len(vocab),\r\n",
        "    \"num_decoding_vocab\": len(vocab),\r\n",
        "    \"num_sequence\": 64, \r\n",
        "    \"num_encoding_sequence\": 512,\r\n",
        "    \"num_decoding_sequence\": 512,\r\n",
        "    \"num_layers\": 12,\r\n",
        "    \"hidden_size\": 128,\r\n",
        "    \"i_pad\": 0,\r\n",
        "    \"d_ff\": 3072,\r\n",
        "    \"n_head\": 12,\r\n",
        "    \"d_head\": 64,\r\n",
        "    \"dropout\": 0.1,\r\n",
        "    \"layer_norm_epsilon\": 1e-12,\r\n",
        "    \"output_size\": 2,\r\n",
        "    \"weight_decay\": 0,\r\n",
        "    \"learning_rate\": 5e-5,\r\n",
        "    \"max_norm\": 5,\r\n",
        "    \"adam_epsilon\": 1e-8,\r\n",
        "    \"warmup_steps\": 0,\r\n",
        "    \"nb_epochs\": 100\r\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50vEbOQXwvS4",
        "outputId": "46df0cf6-3fe8-445b-865e-8ef473168d89"
      },
      "source": [
        "trn_dataset = CustomDataSet(vocab, \"/content/drive/My Drive/data/ratings_train.json\")\r\n",
        "trn_dataloader = data.DataLoader(trn_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=movie_collate_fn)\r\n",
        "test_dataset = CustomDataSet(vocab, \"/content/drive/My Drive/data/ratings_test.json\")\r\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=movie_collate_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/drive/My Drive/data/ratings_train.json: 100%|██████████| 149995/149995 [00:05<00:00, 26452.55 lines/s]\n",
            "Loading /content/drive/My Drive/data/ratings_test.json: 100%|██████████| 49997/49997 [00:02<00:00, 24821.27 lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivnLP6J0j7GB",
        "outputId": "b1a3e09f-5c56-463d-e7c4-19a9c23cc998"
      },
      "source": [
        "trn_label, trn_enc_inputs, trn_dec_inputs = checkdata(trn_dataloader)\r\n",
        "test_label, test_enc_inputs, test_dec_inputs = checkdata(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Batch\n",
            "Input: torch.Size([64, 88])\n",
            "Target: torch.Size([64, 1])\n",
            "1 Batch\n",
            "Input: torch.Size([64, 100])\n",
            "Target: torch.Size([64, 1])\n",
            "2 Batch\n",
            "Input: torch.Size([64, 92])\n",
            "Target: torch.Size([64, 1])\n",
            "3 Batch\n",
            "Input: torch.Size([64, 79])\n",
            "Target: torch.Size([64, 1])\n",
            "4 Batch\n",
            "Input: torch.Size([64, 97])\n",
            "Target: torch.Size([64, 1])\n",
            "5 Batch\n",
            "Input: torch.Size([64, 87])\n",
            "Target: torch.Size([64, 1])\n",
            "0 Batch\n",
            "Input: torch.Size([64, 97])\n",
            "Target: torch.Size([64, 1])\n",
            "1 Batch\n",
            "Input: torch.Size([64, 95])\n",
            "Target: torch.Size([64, 1])\n",
            "2 Batch\n",
            "Input: torch.Size([64, 86])\n",
            "Target: torch.Size([64, 1])\n",
            "3 Batch\n",
            "Input: torch.Size([64, 58])\n",
            "Target: torch.Size([64, 1])\n",
            "4 Batch\n",
            "Input: torch.Size([64, 84])\n",
            "Target: torch.Size([64, 1])\n",
            "5 Batch\n",
            "Input: torch.Size([64, 84])\n",
            "Target: torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPIWBiZnEUXN",
        "outputId": "f9802592-408e-4ffb-f033-5d1783bd6280"
      },
      "source": [
        "print(\"Train label:\", trn_label[0])\r\n",
        "print(\"Train encoder input:\", trn_enc_inputs[0])\r\n",
        "print(\"Train decoder input:\", trn_dec_inputs[0])\r\n",
        "print(\"Test label:\", test_label[0])\r\n",
        "print(\"Test encoder input:\", test_enc_inputs[0])\r\n",
        "print(\"Test decoder input:\", test_dec_inputs[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train label: tensor(1)\n",
            "Train encoder input: tensor([1225,   23, 3589, 3778, 3931, 3760, 1403, 3835, 3596, 3857, 3760, 3587,\n",
            "        7838,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0])\n",
            "Train decoder input: tensor([2])\n",
            "Test label: tensor(0)\n",
            "Test encoder input: tensor([3587, 5053, 3953, 3640, 1803, 2711,  339,   48,  818,  952, 1598,  121,\n",
            "        3605, 3768, 3587, 5053, 3953, 3591,  226,    9,  212, 3611, 3768,  115,\n",
            "          26, 3794, 3750,  748,   26, 4973, 3602, 1920, 1920, 3686, 3877, 3804,\n",
            "        4014, 2039, 3699, 3996, 4502,  228, 3596, 4282, 3766, 3756, 4353, 1920,\n",
            "        1920,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
            "Test decoder input: tensor([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7dveVttE1qw"
      },
      "source": [
        "encoder = Encoder(config)\r\n",
        "decoder = Decoder(config)\r\n",
        "\r\n",
        "trn_enc_outputs, trn_enc_attn_probs = encoder(trn_enc_inputs)\r\n",
        "trn_dec_outputs, trn_self_attn_probs, trn_dec_enc_attn_probs = decoder(trn_dec_inputs, trn_enc_inputs, trn_enc_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JLfQYKGE4O1",
        "outputId": "09127f7a-ff08-4d60-b192-e4b0a724b835"
      },
      "source": [
        "print(encoder)\r\n",
        "print(decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder(\n",
            "  (enc_emb): Embedding(8007, 128)\n",
            "  (pos_emb): Embedding(513, 128)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (1): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (2): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (3): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (4): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (5): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (6): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (7): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (8): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (9): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (10): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (11): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Decoder(\n",
            "  (dec_embs): Embedding(8007, 128)\n",
            "  (pos_emb): Embedding(513, 128)\n",
            "  (layers): ModuleList(\n",
            "    (0): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (1): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (2): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (3): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (4): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (5): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (6): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (7): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (8): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (9): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (10): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (11): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dec_enc_attn): MultiHeadAttention(\n",
            "        (W_Q): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_K): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (W_V): Linear(in_features=128, out_features=768, bias=True)\n",
            "        (scaled_dot_attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (pos_ffn): PoswiseFeedForwardNet(\n",
            "        (conv1): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
            "        (conv2): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b07HsKPiG_w2",
        "outputId": "415d11f9-8e87-429b-c004-db01b5e09c67"
      },
      "source": [
        "print(trn_enc_outputs[0])\r\n",
        "print(trn_enc_attn_probs[0][0][0])\r\n",
        "print(trn_dec_outputs[0])\r\n",
        "print(trn_self_attn_probs[0][0][0])\r\n",
        "print(trn_dec_enc_attn_probs[0][0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3312, -1.0019, -0.7045,  ..., -0.4869, -0.2089, -0.0589],\n",
            "        [ 0.1483, -1.0929,  0.1269,  ...,  1.4843, -1.2941,  0.1250],\n",
            "        [-2.4093, -1.1495, -1.2197,  ...,  0.6273, -0.0952,  0.6801],\n",
            "        ...,\n",
            "        [ 0.1422,  0.2337, -0.9410,  ...,  0.8403, -0.4742, -0.4359],\n",
            "        [-0.3164,  0.5324, -0.6484,  ...,  0.7306, -0.7952, -0.1375],\n",
            "        [-0.5716,  0.7938, -1.1231,  ...,  1.0023, -0.3596, -0.2672]],\n",
            "       grad_fn=<SelectBackward>)\n",
            "tensor([[0.2411, 0.0468, 0.1369,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.1681, 0.0570, 0.2088,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.1639, 0.0919, 0.1126,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0694, 0.0772, 0.0676,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0653, 0.0762, 0.0677,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0673, 0.0748, 0.0706,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<SelectBackward>)\n",
            "tensor([[ 8.2483e-01, -1.6040e-01, -2.2424e-01,  4.3319e-02,  3.0983e-01,\n",
            "          3.1724e-01, -7.3935e-01, -1.1463e+00,  1.6012e+00,  2.9373e-01,\n",
            "         -7.2798e-01,  2.5058e+00, -1.3221e+00, -8.6183e-01, -4.2585e-01,\n",
            "          1.9544e+00, -8.5345e-01,  1.0011e+00, -7.4165e-01,  1.4076e+00,\n",
            "         -1.2190e+00,  6.2164e-01,  1.1099e-01, -3.9096e-01, -5.2156e-01,\n",
            "          1.0321e+00, -1.3225e+00,  1.0139e+00, -8.4114e-01, -1.3863e+00,\n",
            "          1.0231e+00, -1.6270e+00,  3.7259e-02,  1.7172e+00,  4.2984e-01,\n",
            "         -4.3474e-01,  9.9562e-01,  5.6130e-01, -1.0035e+00, -4.4146e-01,\n",
            "         -5.3581e-01,  6.3675e-01,  4.9878e-01, -1.9141e-01,  5.8255e-01,\n",
            "         -7.5224e-01, -1.2113e+00,  3.5333e-01,  1.1770e-01, -1.9182e-01,\n",
            "         -2.0084e-01, -4.9937e-01,  1.3097e-01, -1.1530e+00, -5.4448e-01,\n",
            "          6.6163e-01,  7.0800e-01,  1.0475e+00,  1.8732e+00,  1.1503e+00,\n",
            "          3.4616e-01, -1.7372e-01, -4.2518e-01, -3.2315e-01,  6.3447e-01,\n",
            "         -9.6144e-01, -1.8099e+00, -2.0458e+00, -1.0122e+00,  1.4478e+00,\n",
            "         -2.1544e-01,  1.8271e+00, -3.0299e-01,  7.1684e-01,  1.6295e-01,\n",
            "          2.8069e-01, -5.3543e-01,  1.8328e-01,  1.1273e+00, -2.9837e-01,\n",
            "         -1.0763e+00,  3.2838e-01, -3.1071e-01, -1.6011e-01, -8.8372e-01,\n",
            "         -6.0393e-01,  2.9880e-01, -4.4730e-01,  3.9508e-01,  3.1797e-01,\n",
            "         -1.1269e+00,  5.8224e-01, -1.7323e-02,  4.5981e-01, -5.0152e-01,\n",
            "          1.1037e+00,  2.7913e+00,  1.7160e+00, -8.9119e-01,  1.2566e+00,\n",
            "         -7.1452e-01, -1.5059e-01, -1.3854e-03,  1.4705e-02, -1.2254e+00,\n",
            "         -1.2258e+00, -2.1264e+00,  1.2854e+00, -1.3034e+00, -1.3426e+00,\n",
            "          1.5801e-01,  1.6296e+00,  1.5242e+00, -2.0410e-01,  1.4578e+00,\n",
            "          4.3275e-01, -1.1988e+00,  4.8200e-01, -8.1184e-01, -3.9848e-01,\n",
            "          2.0754e-01, -3.9834e-01,  1.8153e+00, -7.9005e-01, -9.0671e-01,\n",
            "          1.0286e+00, -2.7620e+00, -2.2114e-01]], grad_fn=<SelectBackward>)\n",
            "tensor([[1.1111]], grad_fn=<SelectBackward>)\n",
            "tensor([[0.0000, 0.0521, 0.0877, 0.0961, 0.0000, 0.0729, 0.0463, 0.0913, 0.0909,\n",
            "         0.1164, 0.0864, 0.1299, 0.0673, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aal-nQHVE27m",
        "outputId": "8fa33ca3-ab1f-4d91-d95d-b1bd6115a93f"
      },
      "source": [
        "print(np.shape(trn_enc_outputs))\r\n",
        "print(np.shape(trn_enc_attn_probs))\r\n",
        "print(np.shape(trn_dec_outputs))\r\n",
        "print(np.shape(trn_self_attn_probs))\r\n",
        "print(np.shape(trn_dec_enc_attn_probs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 87, 128])\n",
            "(12,)\n",
            "torch.Size([64, 1, 128])\n",
            "(12,)\n",
            "(12,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekLZQnYMj7GB"
      },
      "source": [
        "model = Classification(config).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = config['learning_rate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDGawJYDB5hG"
      },
      "source": [
        "hypothesis = model(trn_enc_inputs, trn_dec_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVOgxwgON0M9",
        "outputId": "f7ed9f7d-ec77-47ec-912f-ba80caccb5ff"
      },
      "source": [
        "hypothesis[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6885, -1.1506],\n",
              "        [ 0.9400,  0.3825],\n",
              "        [ 0.8728,  0.6479],\n",
              "        [ 0.8540, -0.0659],\n",
              "        [ 0.5589,  0.2431],\n",
              "        [ 1.1899, -0.3123],\n",
              "        [ 0.6480, -0.0479],\n",
              "        [ 1.1072,  0.3918],\n",
              "        [ 1.3039, -0.1402],\n",
              "        [ 0.9356,  0.3019],\n",
              "        [ 1.1641,  0.4604],\n",
              "        [ 1.4840, -0.7345],\n",
              "        [ 1.2879, -0.2980],\n",
              "        [ 0.6916,  0.8984],\n",
              "        [ 0.8344,  0.1325],\n",
              "        [ 1.1648,  0.1282],\n",
              "        [ 0.7323, -0.1503],\n",
              "        [ 1.0404,  0.5493],\n",
              "        [ 0.9016, -0.1578],\n",
              "        [ 0.7259, -0.5757],\n",
              "        [ 0.6604, -0.0790],\n",
              "        [ 0.6236,  0.0972],\n",
              "        [ 1.2589,  0.3647],\n",
              "        [ 0.4845, -0.1122],\n",
              "        [ 0.6667, -0.0321],\n",
              "        [ 1.0663, -0.4645],\n",
              "        [ 0.4871, -0.1381],\n",
              "        [ 0.2773,  0.5033],\n",
              "        [ 0.3979, -0.3347],\n",
              "        [ 1.1419, -0.4560],\n",
              "        [ 0.5753, -0.7924],\n",
              "        [ 0.8543,  0.7123],\n",
              "        [ 1.5589, -0.1648],\n",
              "        [ 1.0534,  0.3961],\n",
              "        [ 0.9457,  0.2923],\n",
              "        [ 0.9636,  0.1614],\n",
              "        [ 0.9595,  0.7571],\n",
              "        [ 0.7330, -0.1564],\n",
              "        [ 1.1474, -0.0762],\n",
              "        [ 0.9146,  0.1388],\n",
              "        [ 1.4512, -0.2001],\n",
              "        [ 1.1137,  0.1133],\n",
              "        [ 0.0855, -0.1302],\n",
              "        [ 0.7588,  0.0910],\n",
              "        [ 1.6006, -0.5161],\n",
              "        [ 0.7553,  0.4419],\n",
              "        [ 0.7765,  0.2248],\n",
              "        [ 1.3281,  0.1312],\n",
              "        [ 0.9033,  0.2172],\n",
              "        [ 0.8001, -0.2994],\n",
              "        [ 0.9545, -0.2011],\n",
              "        [ 0.9374,  0.3377],\n",
              "        [ 0.3924,  0.2257],\n",
              "        [ 0.9477, -0.3141],\n",
              "        [ 1.0903, -0.6759],\n",
              "        [ 1.0216, -0.0872],\n",
              "        [ 1.0085, -0.0522],\n",
              "        [ 1.6489,  0.2531],\n",
              "        [ 0.1500, -0.5811],\n",
              "        [ 1.8539,  0.3604],\n",
              "        [ 0.7903, -0.0119],\n",
              "        [ 0.9787, -0.2352],\n",
              "        [ 1.5437,  0.3573],\n",
              "        [ 0.8857,  0.6996]], grad_fn=<MmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPyTd5bbN5gQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNQzGVaSN5iY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "aLtQ0GKej7GB"
      },
      "source": [
        "#print(\"Model:\", model)\n",
        "print(\"Criterion:\", criterion)\n",
        "print(\"Optimizer:\", optimizer)\n",
        "print(checkfunction(criterion = criterion, \n",
        "                    dataloader = trn_dataloader, \n",
        "                    output_size = config['output_size'], epoch = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op005dhnj7GB"
      },
      "source": [
        "trn_loss, val_loss = train(model, criterion, optimizer,input_size, output_size,\n",
        "                           trn_dataloader, val_dataloader, nb_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb9-kcwfNnAz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}